# -*- coding: utf-8 -*-
"""
ETHUSDT æ™‚é–“æ¡†æ¶ç‰¹æ€§åˆ†æå·¥å…·
=================================================
åŠŸèƒ½ï¼š
1) è‡ªå‹•å¾ Binance æŠ“å– ETHUSDT 1åˆ†é˜æ­·å²è³‡æ–™
2) å¾ 1 åˆ†é˜ OHLCV CSV é‡æ¡æ¨£æˆå¤šå€‹æ™‚é–“æ¡†æ¶ (1m/5m/15m/1h/4h/1d/1w)
3) è¨ˆç®—æ™‚é–“æ¡†æ¶ç‰¹æ€§æŒ‡æ¨™ï¼š
   - æˆæœ¬/æ³¢å‹•æ¯” C/Aï¼ˆCost-to-ATRï¼‰
   - èµ°å‹¢ä¸€è‡´æ€§ï¼ˆVariance Ratio, VRï¼‰
   - è¨Šè™ŸåŠè¡°æœŸï¼ˆåŸºæ–¼å ±é…¬è‡ªç›¸é—œçš„è¿‘ä¼¼ï¼‰
4) ç”Ÿæˆæ™‚é–“æ¡†æ¶ç‰¹æ€§åˆ†æå ±å‘Š
5) åŒ¯å‡ºæ¯å€‹æ™‚é–“æ¡†æ¶çš„å½™æ•´æŒ‡æ¨™åˆ° CSVã€TXTã€MD æ ¼å¼

ä½¿ç”¨æ–¹å¼ï¼š
- è¨­å®š CONFIG å€çš„è³‡æ–™æŠ“å–åƒæ•¸å¾ŒåŸ·è¡Œæœ¬ç¨‹å¼
- ç¨‹å¼æœƒè‡ªå‹•å¾ Binance æŠ“å– ETHUSDT 1åˆ†é˜æ­·å²è³‡æ–™
- ç”¢å‡ºæª”æ¡ˆï¼š./data/ethusdt_timeframe_report_YYYYMMDD-YYYYMMDD.csv
- ç”¢å‡ºæª”æ¡ˆï¼š./data/ethusdt_timeframe_report_YYYYMMDD-YYYYMMDD.txt
- ç”¢å‡ºæª”æ¡ˆï¼š./data/ethusdt_timeframe_report_YYYYMMDD-YYYYMMDD.md

æ³¨æ„ï¼š
- æœ¬å·¥å…·å°ˆæ³¨æ–¼åˆ†æå„æ™‚é–“æ¡†æ¶çš„å¸‚å ´ç‰¹æ€§ï¼Œä¸åŒ…å«ç­–ç•¥å›æ¸¬
- åˆ†æçµæœåŸºæ–¼æŠ€è¡“æŒ‡æ¨™ï¼Œæä¾›æ™‚é–“æ¡†æ¶é¸æ“‡çš„å®¢è§€åƒè€ƒ
"""

import math
import warnings
import requests
import time
import os
from datetime import datetime, timedelta
from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional

import numpy as np
import pandas as pd

warnings.filterwarnings("ignore")


# =======================
# ======== CONFIG =======
# =======================

@dataclass
class Config:
    # è³‡æ–™æŠ“å–è¨­å®š
    symbol: str = "ETHUSDT"                    # äº¤æ˜“å°
    exchange: str = "binance"                  # äº¤æ˜“æ‰€ (ç›®å‰æ”¯æ´ binance)
    data_days: int = 1095                      # è¦æŠ“å–çš„å¤©æ•¸ (3å¹´)
    auto_fetch: bool = True                    # æ˜¯å¦è‡ªå‹•æŠ“å–è³‡æ–™
    save_csv: bool = True                      # æ˜¯å¦å„²å­˜CSVæª”æ¡ˆ
    
    # è³‡æ–™ç®¡ç†è¨­å®š
    force_redownload: bool = False             # å¼·åˆ¶é‡æ–°ä¸‹è¼‰ï¼ˆè¦†è“‹ç¾æœ‰è³‡æ–™ï¼‰
    incremental_update: bool = True            # å¢é‡æ›´æ–°ï¼ˆåªä¸‹è¼‰ç¼ºå¤±çš„è³‡æ–™ï¼‰
    data_quality_check: bool = True            # å•Ÿç”¨è³‡æ–™å“è³ªæª¢æŸ¥
    min_data_quality_score: float = 0.8        # æœ€ä½è³‡æ–™å“è³ªåˆ†æ•¸ï¼ˆ0-1ï¼‰
    
    # å ±å‘Šæ ¼å¼è¨­å®š
    generate_csv_report: bool = True           # ç”ŸæˆCSVå ±å‘Š
    generate_txt_report: bool = True           # ç”ŸæˆTXTå ±å‘Š
    generate_md_report: bool = True            # ç”ŸæˆMDå ±å‘Š
    
    # CSV æª”æ¡ˆè·¯å¾‘ï¼ˆå¦‚æœ auto_fetch=False å‰‡ä½¿ç”¨æ­¤è·¯å¾‘ï¼‰
    csv_path: str = "./data/ethusdt_1m.csv"

    # CSV æ¬„ä½åï¼ˆå¤§å°å¯«èˆ‡å¯¦éš›ä¸€è‡´æˆ–ç”± detect_columns() è‡ªå‹•å°æ˜ ï¼‰
    ts_col: str = "timestamp"
    open_col: str = "open"
    high_col: str = "high"
    low_col: str = "low"
    close_col: str = "close"
    vol_col: str = "volume"

    # æ™‚å€å­—ä¸²ï¼ˆè‹¥ timestamp å·²ç‚º UTCï¼Œå¯ç¶­æŒ "UTC"ï¼‰
    tz: str = "UTC"

    # å€™é¸æ™‚é–“æ¡†æ¶ï¼ˆpandas resample è¦å‰‡ï¼‰
    # 1min å·²æ˜¯ä¾†æºé »ç‡ï¼Œä¿ç•™ä»¥ä¾¿çµ±ä¸€æµç¨‹
    timeframes: Dict[str, str] = None

    # æˆæœ¬å‡è¨­ï¼ˆå–®é‚Šï¼Œä¸€æ¬¡æˆäº¤ï¼‰
    taker_fee: float = 0.0004     # 0.04%
    maker_fee: float = 0.0002     # 0.02%
    slippage_bps: float = 2.0     # 2 bps = 0.02%
    use_taker: bool = True        # True: ç”¨åƒå–®è²»ç‡ï¼›False: ç”¨æ›å–®è²»ç‡

    # ATR åƒæ•¸
    atr_period: int = 14

    # Variance Ratio åƒæ•¸ï¼ˆä»¥ q è¡¨ç¤ºèšåˆå°ºåº¦ï¼‰
    vr_q: int = 4

    # è¨Šè™ŸåŠè¡°æœŸè¨ˆç®—çš„æœ€å¤§å»¶é²ï¼ˆå–®ä½ï¼šbarï¼‰
    half_life_max_lag: int = 100

    # å‹•æ…‹æœ€å°è³‡æ–™é‡è¨­å®š
    use_dynamic_min_bars: bool = True          # æ˜¯å¦ä½¿ç”¨å‹•æ…‹æœ€å°è³‡æ–™é‡
    n_min_bars_for_backtest: int = 1500       # å›ºå®šæœ€å° bar æ•¸ï¼ˆç•¶ use_dynamic_min_bars=False æ™‚ä½¿ç”¨ï¼‰
    
    # å„æ™‚é–“æ¡†æ¶çš„æœ€å°è³‡æ–™å¤©æ•¸è¦æ±‚
    min_days_per_timeframe: Dict[str, int] = None

    def __post_init__(self):
        if self.timeframes is None:
            self.timeframes = {
                "1m": "1T",
                "5m": "5T", 
                "15m": "15T",
                "1h": "1H",
                "4h": "4H",
                "1d": "1D",
                "1w": "1W"
            }
        
        if self.min_days_per_timeframe is None:
            self.min_days_per_timeframe = {
                "1m": 30,    # 1åˆ†é˜éœ€è¦30å¤©
                "5m": 60,    # 5åˆ†é˜éœ€è¦60å¤©
                "15m": 90,   # 15åˆ†é˜éœ€è¦90å¤©
                "1h": 180,   # 1å°æ™‚éœ€è¦180å¤©
                "4h": 365,   # 4å°æ™‚éœ€è¦365å¤©
                "1d": 730,   # 1å¤©éœ€è¦730å¤©
                "1w": 1095   # 1é€±éœ€è¦1095å¤©
            }


CFG = Config()


# =======================
# ====== DATA FETCH =====
# =======================

def fetch_binance_klines(symbol: str, interval: str, start_time: int, end_time: int) -> List[List]:
    """
    å¾ Binance API æŠ“å– Kç·šè³‡æ–™
    
    Args:
        symbol: äº¤æ˜“å° (å¦‚ "ETHUSDT")
        interval: æ™‚é–“é–“éš” ("1m", "5m", "1h", "1d" ç­‰)
        start_time: é–‹å§‹æ™‚é–“æˆ³è¨˜ (æ¯«ç§’)
        end_time: çµæŸæ™‚é–“æˆ³è¨˜ (æ¯«ç§’)
    
    Returns:
        Kç·šè³‡æ–™åˆ—è¡¨
    """
    url = "https://api.binance.com/api/v3/klines"
    params = {
        "symbol": symbol,
        "interval": interval,
        "startTime": start_time,
        "endTime": end_time,
        "limit": 1000  # Binance å–®æ¬¡è«‹æ±‚æœ€å¤§é™åˆ¶
    }
    
    try:
        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"æŠ“å–è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return []


def fetch_historical_data(symbol: str, days: int, interval: str = "1m") -> pd.DataFrame:
    """
    æŠ“å–æŒ‡å®šå¤©æ•¸çš„æ­·å²è³‡æ–™
    
    Args:
        symbol: äº¤æ˜“å°
        days: è¦æŠ“å–çš„å¤©æ•¸
        interval: æ™‚é–“é–“éš”
    
    Returns:
        åŒ…å« OHLCV è³‡æ–™çš„ DataFrame
    """
    print(f"é–‹å§‹å¾ Binance æŠ“å– {symbol} {interval} è³‡æ–™ï¼Œå…± {days} å¤©...")
    
    # è¨ˆç®—æ™‚é–“ç¯„åœ
    end_time = int(time.time() * 1000)  # ç¾åœ¨æ™‚é–“ (æ¯«ç§’)
    start_time = end_time - (days * 24 * 60 * 60 * 1000)  # days å¤©å‰
    
    all_data = []
    current_start = start_time
    
    while current_start < end_time:
        current_end = min(current_start + (1000 * 60 * 1000), end_time)  # æ¯æ¬¡æœ€å¤š1000æ ¹Kç·š
        
        print(f"æŠ“å– {datetime.fromtimestamp(current_start/1000)} åˆ° {datetime.fromtimestamp(current_end/1000)} çš„è³‡æ–™...")
        
        klines = fetch_binance_klines(symbol, interval, current_start, current_end)
        
        if not klines:
            print("è­¦å‘Šï¼šæ­¤æ™‚é–“ç¯„åœæ²’æœ‰è³‡æ–™ï¼Œè·³é...")
            current_start = current_end
            continue
        
        all_data.extend(klines)
        current_start = current_end
        
        # é¿å…è«‹æ±‚éæ–¼é »ç¹
        time.sleep(0.1)
    
    if not all_data:
        raise ValueError("æ²’æœ‰æŠ“å–åˆ°ä»»ä½•è³‡æ–™")
    
    # è½‰æ›ç‚º DataFrame
    df = pd.DataFrame(all_data, columns=[
        'timestamp', 'open', 'high', 'low', 'close', 'volume',
        'close_time', 'quote_asset_volume', 'number_of_trades',
        'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'
    ])
    
    # è½‰æ›è³‡æ–™é¡å‹
    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
    for col in ['open', 'high', 'low', 'close', 'volume']:
        df[col] = df[col].astype(float)
    
    # è¨­å®šæ™‚å€
    df['timestamp'] = df['timestamp'].dt.tz_localize('UTC')
    
    # æ’åºä¸¦å»é‡
    df = df.sort_values('timestamp').drop_duplicates(subset=['timestamp'])
    df = df.set_index('timestamp')
    
    print(f"æˆåŠŸæŠ“å– {len(df)} æ ¹Kç·šè³‡æ–™")
    print(f"è³‡æ–™æ™‚é–“ç¯„åœ: {df.index.min()} åˆ° {df.index.max()}")
    
    return df


def save_data_to_csv(df: pd.DataFrame, filepath: str) -> None:
    """å°‡è³‡æ–™å„²å­˜ç‚º CSV æª”æ¡ˆ"""
    try:
        df.to_csv(filepath, encoding='utf-8-sig')
        print(f"è³‡æ–™å·²å„²å­˜è‡³: {filepath}")
    except Exception as e:
        print(f"å„²å­˜CSVæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")


def load_or_fetch_data(cfg: Config) -> pd.DataFrame:
    """
    æ ¹æ“šè¨­å®šè¼‰å…¥æˆ–æŠ“å–è³‡æ–™
    
    Returns:
        åŒ…å« OHLCV è³‡æ–™çš„ DataFrame
    """
    if cfg.auto_fetch:
        print("=== è‡ªå‹•æŠ“å–æ¨¡å¼ ===")
        try:
            df = fetch_historical_data(cfg.symbol, cfg.data_days)
            
            if cfg.save_csv:
                save_data_to_csv(df, cfg.csv_path)
            
            return df
            
        except Exception as e:
            print(f"è‡ªå‹•æŠ“å–å¤±æ•—: {e}")
            print("å˜—è©¦ä½¿ç”¨æœ¬åœ°CSVæª”æ¡ˆ...")
            return load_1m_csv(cfg.csv_path, cfg)
    else:
        print("=== æœ¬åœ°CSVæ¨¡å¼ ===")
        return load_1m_csv(cfg.csv_path, cfg)


def check_existing_data(cfg: Config) -> Tuple[bool, pd.DataFrame, Dict]:
    """
    æª¢æŸ¥ç¾æœ‰è³‡æ–™çš„ç‹€æ…‹
    
    Returns:
        (è³‡æ–™æ˜¯å¦å­˜åœ¨, DataFrame, è³‡æ–™ç‹€æ…‹å ±å‘Š)
    """
    import os
    
    if not os.path.exists(cfg.csv_path):
        return False, None, {"status": "æª”æ¡ˆä¸å­˜åœ¨"}
    
    try:
        df = load_1m_csv(cfg.csv_path, cfg)
        if df.empty:
            return False, df, {"status": "æª”æ¡ˆå­˜åœ¨ä½†ç‚ºç©º"}
        
        # è¨ˆç®—è³‡æ–™ç‹€æ…‹
        start_time = df.index.min()
        end_time = df.index.max()
        total_days = (end_time - start_time).total_seconds() / (24 * 3600)
        expected_days = cfg.data_days
        
        status = {
            "status": "è³‡æ–™å­˜åœ¨",
            "start_time": start_time,
            "end_time": end_time,
            "total_days": total_days,
            "expected_days": expected_days,
            "data_completeness": min(total_days / expected_days, 1.0),
            "total_bars": len(df),
            "missing_bars": 0,
            "duplicate_bars": df.index.duplicated().sum(),
            "null_values": df.isnull().sum().sum()
        }
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ç¼ºå¤±çš„æ™‚é–“é»
        expected_bars = int(expected_days * 24 * 60)  # é æœŸçš„åˆ†é˜æ•¸
        status["missing_bars"] = max(0, expected_bars - len(df))
        
        return True, df, status
        
    except Exception as e:
        return False, None, {"status": f"è®€å–å¤±æ•—: {e}"}


def check_data_quality(df: pd.DataFrame, cfg: Config) -> Dict:
    """
    æª¢æŸ¥è³‡æ–™å“è³ªä¸¦ç”¢ç”Ÿå ±å‘Š
    
    Returns:
        è³‡æ–™å“è³ªå ±å‘Šå­—å…¸
    """
    if df is None or df.empty:
        return {"quality_score": 0.0, "issues": ["è³‡æ–™ç‚ºç©º"]}
    
    issues = []
    score = 1.0
    
    # 1. æª¢æŸ¥åŸºæœ¬çµæ§‹
    required_cols = ['open', 'high', 'low', 'close', 'volume']
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        issues.append(f"ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_cols}")
        score -= 0.3
    
    # 2. æª¢æŸ¥æ™‚é–“åºåˆ—å®Œæ•´æ€§
    time_diff = df.index.to_series().diff().dropna()
    expected_diff = pd.Timedelta(minutes=1)
    irregular_intervals = (time_diff != expected_diff).sum()
    if irregular_intervals > 0:
        issues.append(f"æ™‚é–“é–“éš”ä¸è¦å‰‡: {irregular_intervals} è™•")
        score -= min(0.2, irregular_intervals / len(df) * 0.5)
    
    # 3. æª¢æŸ¥é‡è¤‡è³‡æ–™
    duplicates = df.index.duplicated().sum()
    if duplicates > 0:
        issues.append(f"é‡è¤‡æ™‚é–“æˆ³è¨˜: {duplicates} å€‹")
        score -= min(0.2, duplicates / len(df) * 0.5)
    
    # 4. æª¢æŸ¥ç¼ºå¤±å€¼
    null_counts = df.isnull().sum()
    total_nulls = null_counts.sum()
    if total_nulls > 0:
        issues.append(f"ç¼ºå¤±å€¼: {total_nulls} å€‹")
        score -= min(0.2, total_nulls / (len(df) * len(df.columns)) * 0.5)
    
    # 5. æª¢æŸ¥åƒ¹æ ¼é‚è¼¯
    price_errors = ((df['high'] < df['low']) | 
                   (df['open'] > df['high']) | 
                   (df['close'] > df['high']) |
                   (df['open'] < df['low']) | 
                   (df['close'] < df['low'])).sum()
    if price_errors > 0:
        issues.append(f"åƒ¹æ ¼é‚è¼¯éŒ¯èª¤: {price_errors} ç­†")
        score -= min(0.3, price_errors / len(df) * 0.5)
    
    # 6. æª¢æŸ¥ç•°å¸¸å€¼
    for col in ['open', 'high', 'low', 'close']:
        if col in df.columns:
            # æª¢æŸ¥æ˜¯å¦æœ‰é›¶å€¼æˆ–è² å€¼
            zero_or_negative = (df[col] <= 0).sum()
            if zero_or_negative > 0:
                issues.append(f"{col} æ¬„ä½æœ‰é›¶å€¼æˆ–è² å€¼: {zero_or_negative} ç­†")
                score -= min(0.1, zero_or_negative / len(df) * 0.3)
    
    # 7. æª¢æŸ¥æˆäº¤é‡
    if 'volume' in df.columns:
        negative_volume = (df['volume'] < 0).sum()
        if negative_volume > 0:
            issues.append(f"è² æˆäº¤é‡: {negative_volume} ç­†")
            score -= min(0.1, negative_volume / len(df) * 0.3)
    
    # 8. æª¢æŸ¥æ™‚é–“ç¯„åœ
    now = pd.Timestamp.now(tz='UTC')
    if df.index.max() > now:
        issues.append("è³‡æ–™åŒ…å«æœªä¾†æ™‚é–“")
        score -= 0.1
    
    score = max(0.0, score)
    
    return {
        "quality_score": score,
        "issues": issues,
        "total_bars": len(df),
        "time_range": f"{df.index.min()} åˆ° {df.index.max()}",
        "data_completeness": len(df) / (cfg.data_days * 24 * 60) if cfg.data_days > 0 else 1.0
    }


def get_missing_date_ranges(df: pd.DataFrame, target_days: int) -> List[Tuple[pd.Timestamp, pd.Timestamp]]:
    """
    æ‰¾å‡ºç¼ºå¤±çš„æ—¥æœŸç¯„åœ
    
    Returns:
        ç¼ºå¤±æ—¥æœŸç¯„åœåˆ—è¡¨ [(start, end), ...]
    """
    if df is None or df.empty:
        # å¦‚æœæ²’æœ‰è³‡æ–™ï¼Œè¿”å›å®Œæ•´çš„ç›®æ¨™ç¯„åœ
        end_time = pd.Timestamp.now(tz='UTC')
        start_time = end_time - pd.Timedelta(days=target_days)
        return [(start_time, end_time)]
    
    end_time = pd.Timestamp.now(tz='UTC')
    start_time = end_time - pd.Timedelta(days=target_days)
    
    missing_ranges = []
    
    # æª¢æŸ¥é–‹å§‹æ™‚é–“
    if df.index.min() > start_time:
        missing_ranges.append((start_time, df.index.min()))
    
    # æª¢æŸ¥çµæŸæ™‚é–“
    if df.index.max() < end_time:
        missing_ranges.append((df.index.max(), end_time))
    
    # æª¢æŸ¥ä¸­é–“çš„é–“éš™
    time_diff = df.index.to_series().diff().dropna()
    expected_diff = pd.Timedelta(minutes=1)
    gap_indices = time_diff[time_diff > expected_diff].index
    
    for gap_start in gap_indices:
        gap_end = gap_start + time_diff[gap_start]
        missing_ranges.append((gap_start, gap_end))
    
    return missing_ranges


def fetch_incremental_data(df: pd.DataFrame, cfg: Config) -> pd.DataFrame:
    """
    å¢é‡ä¸‹è¼‰ç¼ºå¤±çš„è³‡æ–™
    
    Returns:
        åˆä½µå¾Œçš„å®Œæ•´DataFrame
    """
    print("=== å¢é‡è³‡æ–™æ›´æ–° ===")
    
    missing_ranges = get_missing_date_ranges(df, cfg.data_days)
    
    if not missing_ranges:
        print("è³‡æ–™å·²å®Œæ•´ï¼Œç„¡éœ€å¢é‡æ›´æ–°")
        return df
    
    print(f"ç™¼ç¾ {len(missing_ranges)} å€‹ç¼ºå¤±æ™‚é–“ç¯„åœ")
    
    new_data = []
    for i, (start_time, end_time) in enumerate(missing_ranges):
        print(f"ä¸‹è¼‰ç¼ºå¤±è³‡æ–™ {i+1}/{len(missing_ranges)}: {start_time} åˆ° {end_time}")
        
        start_ms = int(start_time.timestamp() * 1000)
        end_ms = int(end_time.timestamp() * 1000)
        
        klines = fetch_binance_klines(cfg.symbol, "1m", start_ms, end_ms)
        
        if klines:
            # è½‰æ›ç‚ºDataFrame
            temp_df = pd.DataFrame(klines, columns=[
                'timestamp', 'open', 'high', 'low', 'close', 'volume',
                'close_time', 'quote_asset_volume', 'number_of_trades',
                'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'
            ])
            
            temp_df['timestamp'] = pd.to_datetime(temp_df['timestamp'], unit='ms')
            for col in ['open', 'high', 'low', 'close', 'volume']:
                temp_df[col] = temp_df[col].astype(float)
            
            temp_df['timestamp'] = temp_df['timestamp'].dt.tz_localize('UTC')
            temp_df = temp_df.set_index('timestamp')
            
            new_data.append(temp_df)
        
        time.sleep(0.1)  # é¿å…APIé™åˆ¶
    
    if new_data:
        # åˆä½µæ–°è³‡æ–™
        new_df = pd.concat(new_data)
        combined_df = pd.concat([df, new_df])
        combined_df = combined_df.sort_index().drop_duplicates()
        
        print(f"å¢é‡æ›´æ–°å®Œæˆï¼Œæ–°å¢ {len(new_df)} æ ¹Kç·š")
        return combined_df
    else:
        print("æ²’æœ‰æ–°è³‡æ–™éœ€è¦ä¸‹è¼‰")
        return df


def smart_data_loader(cfg: Config) -> pd.DataFrame:
    """
    æ™ºèƒ½è³‡æ–™è¼‰å…¥å™¨ï¼šæª¢æŸ¥ç¾æœ‰è³‡æ–™ï¼Œæ±ºå®šæ˜¯å¦éœ€è¦ä¸‹è¼‰æˆ–æ›´æ–°
    
    Returns:
        å®Œæ•´çš„DataFrame
    """
    print("=== æ™ºèƒ½è³‡æ–™ç®¡ç† ===")
    
    # æª¢æŸ¥ç¾æœ‰è³‡æ–™
    data_exists, existing_df, status = check_existing_data(cfg)
    
    if data_exists:
        print(f"ç™¼ç¾ç¾æœ‰è³‡æ–™: {status['total_bars']} æ ¹Kç·š")
        print(f"æ™‚é–“ç¯„åœ: {status['start_time']} åˆ° {status['end_time']}")
        print(f"è³‡æ–™å®Œæ•´åº¦: {status['data_completeness']:.2%}")
        
        # è³‡æ–™å“è³ªæª¢æŸ¥
        if cfg.data_quality_check:
            quality_report = check_data_quality(existing_df, cfg)
            print(f"è³‡æ–™å“è³ªåˆ†æ•¸: {quality_report['quality_score']:.2f}")
            
            if quality_report['issues']:
                print("ç™¼ç¾è³‡æ–™å•é¡Œ:")
                for issue in quality_report['issues']:
                    print(f"  - {issue}")
            
            if quality_report['quality_score'] < cfg.min_data_quality_score:
                print(f"è³‡æ–™å“è³ªåˆ†æ•¸ ({quality_report['quality_score']:.2f}) ä½æ–¼é–¾å€¼ ({cfg.min_data_quality_score})")
                if not cfg.force_redownload:
                    print("å»ºè­°é‡æ–°ä¸‹è¼‰è³‡æ–™æˆ–æª¢æŸ¥è³‡æ–™ä¾†æº")
        
        # æ±ºå®šæ˜¯å¦éœ€è¦é‡æ–°ä¸‹è¼‰
        if cfg.force_redownload:
            print("å¼·åˆ¶é‡æ–°ä¸‹è¼‰æ¨¡å¼")
            return fetch_historical_data(cfg.symbol, cfg.data_days)
        
        # æª¢æŸ¥æ˜¯å¦éœ€è¦å¢é‡æ›´æ–°
        if cfg.incremental_update and status['data_completeness'] < 0.95:
            print("è³‡æ–™ä¸å®Œæ•´ï¼Œé€²è¡Œå¢é‡æ›´æ–°...")
            return fetch_incremental_data(existing_df, cfg)
        
        print("ä½¿ç”¨ç¾æœ‰è³‡æ–™")
        return existing_df
    
    else:
        print("æ²’æœ‰ç¾æœ‰è³‡æ–™ï¼Œé–‹å§‹ä¸‹è¼‰...")
        return fetch_historical_data(cfg.symbol, cfg.data_days)


def generate_data_report(df: pd.DataFrame, cfg: Config) -> str:
    """
    ç”¢ç”Ÿè©³ç´°çš„è³‡æ–™å ±å‘Š
    
    Returns:
        å ±å‘Šæ–‡å­—
    """
    if df is None or df.empty:
        return "è³‡æ–™å ±å‘Š: ç„¡è³‡æ–™"
    
    quality_report = check_data_quality(df, cfg)
    
    report = []
    report.append("=" * 50)
    report.append("è³‡æ–™å“è³ªå ±å‘Š")
    report.append("=" * 50)
    report.append(f"è³‡æ–™ä¾†æº: {cfg.exchange.upper()}")
    report.append(f"äº¤æ˜“å°: {cfg.symbol}")
    report.append(f"æ™‚é–“ç¯„åœ: {df.index.min()} åˆ° {df.index.max()}")
    report.append(f"ç¸½Kç·šæ•¸: {len(df):,}")
    report.append(f"è³‡æ–™å®Œæ•´åº¦: {quality_report['data_completeness']:.2%}")
    report.append(f"å“è³ªåˆ†æ•¸: {quality_report['quality_score']:.2f}")
    report.append("")
    
    if quality_report['issues']:
        report.append("ç™¼ç¾çš„å•é¡Œ:")
        for issue in quality_report['issues']:
            report.append(f"  âŒ {issue}")
    else:
        report.append("âœ… è³‡æ–™å“è³ªè‰¯å¥½ï¼Œæœªç™¼ç¾å•é¡Œ")
    
    report.append("")
    report.append("åŸºæœ¬çµ±è¨ˆ:")
    for col in ['open', 'high', 'low', 'close']:
        if col in df.columns:
            report.append(f"  {col.upper()}: {df[col].min():.2f} - {df[col].max():.2f}")
    
    if 'volume' in df.columns:
        report.append(f"  Volume: {df['volume'].min():.2f} - {df['volume'].max():.2f}")
    
    report.append("=" * 50)
    
    return "\n".join(report)


def generate_txt_report(report_df: pd.DataFrame, cfg: Config, df_1m: pd.DataFrame) -> str:
    """
    ç”ŸæˆTXTæ ¼å¼çš„è©³ç´°å ±å‘Š
    
    Args:
        report_df: æ™‚é–“æ¡†æ¶åˆ†æçµæœDataFrame
        cfg: é…ç½®ç‰©ä»¶
        df_1m: åŸå§‹1åˆ†é˜è³‡æ–™
    
    Returns:
        TXTæ ¼å¼å ±å‘Šæ–‡å­—
    """
    report = []
    report.append("=" * 80)
    report.append("ETHUSDT æ™‚é–“æ¡†æ¶é¸æ“‡åˆ†æå ±å‘Š")
    report.append("=" * 80)
    report.append("")
    
    # åŸºæœ¬è³‡è¨Š
    report.append("ğŸ“Š åŸºæœ¬è³‡è¨Š")
    report.append("-" * 40)
    report.append(f"äº¤æ˜“å°: {cfg.symbol}")
    report.append(f"äº¤æ˜“æ‰€: {cfg.exchange.upper()}")
    report.append(f"æ¸¬è©¦æ—¥æœŸç¯„åœ: {df_1m.index.min().strftime('%Y-%m-%d %H:%M:%S')} åˆ° {df_1m.index.max().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append(f"ç¸½æ¸¬è©¦å¤©æ•¸: {(df_1m.index.max() - df_1m.index.min()).days} å¤©")
    report.append(f"åŸå§‹è³‡æ–™Kç·šæ•¸: {len(df_1m):,}")
    report.append(f"å ±å‘Šç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append("")
    
    # æˆæœ¬è¨­å®š
    cost_one_way = (cfg.taker_fee if cfg.use_taker else cfg.maker_fee) + cfg.slippage_bps / 10000.0
    report.append("ğŸ’° æˆæœ¬è¨­å®š")
    report.append("-" * 40)
    report.append(f"è²»ç‡é¡å‹: {'åƒå–®è²»ç‡' if cfg.use_taker else 'æ›å–®è²»ç‡'}")
    report.append(f"å–®é‚Šæˆæœ¬: {cost_one_way:.6f} ({cost_one_way*100:.4f}%)")
    report.append(f"ä¾†å›æˆæœ¬: {cost_one_way*2:.6f} ({cost_one_way*2*100:.4f}%)")
    report.append("")
    
    # åˆ†æè¨­å®š
    report.append("âš™ï¸ åˆ†æè¨­å®š")
    report.append("-" * 40)
    report.append(f"ATRé€±æœŸ: {cfg.atr_period}")
    report.append(f"Variance Ratioèšåˆå°ºåº¦: {cfg.vr_q}")
    report.append(f"åŠè¡°æœŸæœ€å¤§å»¶é²: {cfg.half_life_max_lag}")
    report.append("")
    
    # æ™‚é–“æ¡†æ¶åˆ†æçµæœ
    report.append("ğŸ“ˆ æ™‚é–“æ¡†æ¶åˆ†æçµæœ")
    report.append("-" * 40)
    report.append("")
    
    # å‰µå»ºå·²æ¸¬è©¦æ™‚é–“æ¡†æ¶çš„é›†åˆ
    tested_timeframes = set(report_df['Timeframe'].tolist())
    
    # éæ­·æ‰€æœ‰é…ç½®çš„æ™‚é–“æ¡†æ¶
    for tf_label, rule in cfg.timeframes.items():
        if tf_label in tested_timeframes:
            # æ‰¾åˆ°å°æ‡‰çš„æ¸¬è©¦çµæœ
            row = report_df[report_df['Timeframe'] == tf_label].iloc[0]
            report.append(f"ğŸ• {row['Timeframe']} æ™‚é–“æ¡†æ¶")
            report.append(f"    Kç·šæ•¸é‡: {row['Bars']:,}")
            report.append(f"    å¹³å‡ATR: {row['Avg_ATR_pct']:.4f} ({row['Avg_ATR_pct']*100:.2f}%)")
            report.append(f"    æˆæœ¬/æ³¢å‹•æ¯” (C/A): {row['C_over_A']:.4f}")
            report.append(f"    èµ°å‹¢ä¸€è‡´æ€§ (VR): {row['VarianceRatio']:.4f}")
            report.append(f"    è¨Šè™ŸåŠè¡°æœŸ: {row['HalfLife_bars']:.1f} bars")
            report.append(f"    å¹´åŒ–æ³¢å‹•ç‡: {row['Volatility_Ann']:.4f}")
            report.append(f"    å ±é…¬ååº¦: {row['Skewness']:.4f}")
            report.append(f"    å ±é…¬å³°åº¦: {row['Kurtosis']:.4f}")
            report.append(f"    è‡ªç›¸é—œ(Lag1): {row['Autocorr_Lag1']:.4f}")
            report.append(f"    å¸‚å ´æ•ˆç‡æ¯”ç‡: {row['Market_Efficiency']:.4f}")
            
            # å¯è¡Œæ€§è©•ä¼°
            if row['Pass_CA_0.25']:
                report.append(f"    âœ… é€šéC/A < 0.25æ¸¬è©¦")
            else:
                report.append(f"    âŒ æœªé€šéC/A < 0.25æ¸¬è©¦")
        else:
            # æœªæ¸¬è©¦çš„æ™‚é–“æ¡†æ¶
            report.append(f"ğŸ• {tf_label} æ™‚é–“æ¡†æ¶")
            report.append(f"    âŒ æœªé€²è¡Œæ¸¬è©¦")
            min_bars_required = get_min_bars_for_timeframe(tf_label, cfg)
            min_days_required = cfg.min_days_per_timeframe.get(tf_label, 365) if cfg.use_dynamic_min_bars else "N/A"
            report.append(f"    åŸå› : è³‡æ–™é‡ä¸è¶³ï¼ˆéœ€è¦è‡³å°‘ {min_bars_required} æ ¹Kç·šï¼‰")
            if cfg.use_dynamic_min_bars:
                report.append(f"    è¦æ±‚: è‡³å°‘ {min_days_required} å¤©çš„è³‡æ–™")
            report.append(f"    å»ºè­°: å¢åŠ è³‡æ–™å¤©æ•¸æˆ–èª¿æ•´æœ€å°è³‡æ–™é‡è¨­å®š")
        
        report.append("")
    
    # ç¶œåˆå»ºè­°
    report.append("ğŸ’¡ ç¶œåˆå»ºè­°")
    report.append("-" * 40)
    
    # æ‰¾å‡ºæœ€ä½³æ™‚é–“æ¡†æ¶
    best_ca = report_df.loc[report_df['C_over_A'].idxmin()] if 'C_over_A' in report_df.columns else None
    best_vr = report_df.loc[report_df['VarianceRatio'].idxmax()] if 'VarianceRatio' in report_df.columns else None
    
    if best_ca is not None and not pd.isna(best_ca['C_over_A']):
        report.append(f"æœ€ä½³æˆæœ¬æ•ˆç‡æ™‚é–“æ¡†æ¶: {best_ca['Timeframe']} (C/A: {best_ca['C_over_A']:.4f})")
    
    if best_vr is not None and not pd.isna(best_vr['VarianceRatio']):
        report.append(f"æœ€é«˜è¶¨å‹¢æ€§æ™‚é–“æ¡†æ¶: {best_vr['Timeframe']} (VR: {best_vr['VarianceRatio']:.4f})")
    
    report.append("")
    report.append("ğŸ“‹ æŒ‡æ¨™è§£è®€æŒ‡å—:")
    report.append("â€¢ C/A < 0.25: æˆæœ¬ç›¸å°æ–¼æ³¢å‹•ç‡è¼ƒä½ï¼Œé©åˆäº¤æ˜“")
    report.append("â€¢ VR > 1: åè¶¨å‹¢å¸‚å ´ï¼Œé©åˆè¶¨å‹¢ç­–ç•¥")
    report.append("â€¢ VR < 1: åå‡å€¼å›æ­¸å¸‚å ´ï¼Œé©åˆå‡å€¼å›æ­¸ç­–ç•¥")
    report.append("â€¢ åŠè¡°æœŸ: å»ºè­°baré€±æœŸç´„ç‚º0.5~1å€åŠè¡°æœŸ")
    report.append("â€¢ æ³¢å‹•ç‡: åæ˜ å¸‚å ´æ³¢å‹•ç¨‹åº¦")
    report.append("â€¢ ååº¦: æ­£ååº¦è¡¨ç¤ºå³å°¾è¼ƒé•·ï¼Œè² ååº¦è¡¨ç¤ºå·¦å°¾è¼ƒé•·")
    report.append("â€¢ å³°åº¦: é«˜å³°åº¦è¡¨ç¤ºæ¥µç«¯å€¼è¼ƒå¤š")
    report.append("â€¢ è‡ªç›¸é—œ: æ­£å€¼è¡¨ç¤ºè¶¨å‹¢æ€§ï¼Œè² å€¼è¡¨ç¤ºå‡å€¼å›æ­¸")
    report.append("â€¢ å¸‚å ´æ•ˆç‡æ¯”ç‡: è¶Šæ¥è¿‘1è¡¨ç¤ºå¸‚å ´è¶Šæœ‰æ•ˆç‡")
    
    # æ·»åŠ è©³ç´°æŒ‡æ¨™è§£é‡‹
    report.append("")
    report.append("ğŸ“Š è©³ç´°æŒ‡æ¨™è§£é‡‹")
    report.append("-" * 40)
    report.append("")
    
    report.append("ğŸ” æˆæœ¬/æ³¢å‹•æ¯” (C/A Ratio)")
    report.append("æ„ç¾©: è¡¡é‡äº¤æ˜“æˆæœ¬ç›¸å°æ–¼å¸‚å ´æ³¢å‹•çš„æ¯”ç‡")
    report.append("è§£è®€: ")
    report.append("â€¢ C/A < 0.25: æˆæœ¬ç›¸å°è¼ƒä½ï¼Œé©åˆé »ç¹äº¤æ˜“")
    report.append("â€¢ C/A 0.25-0.5: æˆæœ¬é©ä¸­ï¼Œéœ€è¦è¬¹æ…é¸æ“‡å…¥å ´é»")
    report.append("â€¢ C/A > 0.5: æˆæœ¬éé«˜ï¼Œä¸é©åˆçŸ­ç·šäº¤æ˜“")
    report.append("")
    
    report.append("ğŸ” èµ°å‹¢ä¸€è‡´æ€§ (Variance Ratio, VR)")
    report.append("æ„ç¾©: è¡¡é‡åƒ¹æ ¼è®Šå‹•çš„è¶¨å‹¢æ€§å¼·åº¦")
    report.append("è§£è®€:")
    report.append("â€¢ VR > 1: åƒ¹æ ¼è®Šå‹•å…·æœ‰è¶¨å‹¢æ€§ï¼Œé©åˆè¶¨å‹¢è·Ÿéš¨ç­–ç•¥")
    report.append("â€¢ VR < 1: åƒ¹æ ¼è®Šå‹•åå‘éš¨æ©ŸéŠèµ°ï¼Œé©åˆå‡å€¼å›æ­¸ç­–ç•¥")
    report.append("â€¢ VR â‰ˆ 1: åƒ¹æ ¼è®Šå‹•æ¥è¿‘éš¨æ©ŸéŠèµ°")
    report.append("")
    
    report.append("ğŸ” è¨Šè™ŸåŠè¡°æœŸ (Signal Half-Life)")
    report.append("æ„ç¾©: è¡¡é‡åƒ¹æ ¼è¨Šè™Ÿçš„æŒçºŒæ™‚é–“")
    report.append("è§£è®€:")
    report.append("â€¢ åŠè¡°æœŸè¶Šé•·ï¼Œè¨Šè™Ÿè¶ŠæŒä¹…ï¼Œé©åˆè¼ƒé•·æœŸçš„ç­–ç•¥")
    report.append("â€¢ åŠè¡°æœŸè¶ŠçŸ­ï¼Œè¨Šè™Ÿè®ŠåŒ–è¶Šå¿«ï¼Œéœ€è¦æ›´é »ç¹çš„èª¿æ•´")
    report.append("")
    
    report.append("ğŸ” å¹´åŒ–æ³¢å‹•ç‡ (Annualized Volatility)")
    report.append("æ„ç¾©: è¡¡é‡åƒ¹æ ¼è®Šå‹•çš„åŠ‡çƒˆç¨‹åº¦")
    report.append("è§£è®€:")
    report.append("â€¢ æ³¢å‹•ç‡è¶Šé«˜ï¼Œåƒ¹æ ¼è®Šå‹•è¶ŠåŠ‡çƒˆï¼Œé¢¨éšªè¶Šå¤§")
    report.append("â€¢ æ³¢å‹•ç‡è¶Šä½ï¼Œåƒ¹æ ¼è®Šå‹•è¶Šå¹³ç©©ï¼Œé¢¨éšªè¼ƒå°")
    report.append("")
    
    report.append("ğŸ” å ±é…¬ååº¦ (Return Skewness)")
    report.append("æ„ç¾©: è¡¡é‡å ±é…¬åˆ†å¸ƒçš„å°ç¨±æ€§")
    report.append("è§£è®€:")
    report.append("â€¢ æ­£ååº¦: å³å°¾è¼ƒé•·ï¼Œå¤§å¹…ä¸Šæ¼²æ©Ÿç‡è¼ƒé«˜")
    report.append("â€¢ è² ååº¦: å·¦å°¾è¼ƒé•·ï¼Œå¤§å¹…ä¸‹è·Œæ©Ÿç‡è¼ƒé«˜")
    report.append("")
    
    report.append("ğŸ” å ±é…¬å³°åº¦ (Return Kurtosis)")
    report.append("æ„ç¾©: è¡¡é‡å ±é…¬åˆ†å¸ƒçš„å°–éŠ³ç¨‹åº¦")
    report.append("è§£è®€:")
    report.append("â€¢ é«˜å³°åº¦: æ¥µç«¯å€¼å‡ºç¾æ©Ÿç‡è¼ƒé«˜ï¼Œé¢¨éšªè¼ƒå¤§")
    report.append("â€¢ ä½å³°åº¦: åˆ†å¸ƒè¼ƒå¹³å¦ï¼Œæ¥µç«¯å€¼è¼ƒå°‘")
    report.append("")
    
    report.append("ğŸ” è‡ªç›¸é—œ (Autocorrelation)")
    report.append("æ„ç¾©: è¡¡é‡ç•¶å‰åƒ¹æ ¼èˆ‡éå»åƒ¹æ ¼çš„ç›¸é—œæ€§")
    report.append("è§£è®€:")
    report.append("â€¢ æ­£å€¼: åƒ¹æ ¼å…·æœ‰è¶¨å‹¢æ€§ï¼Œéå»èµ°å‹¢å°æœªä¾†æœ‰å½±éŸ¿")
    report.append("â€¢ è² å€¼: åƒ¹æ ¼å…·æœ‰å‡å€¼å›æ­¸ç‰¹æ€§")
    report.append("")
    
    report.append("ğŸ” å¸‚å ´æ•ˆç‡æ¯”ç‡ (Market Efficiency Ratio)")
    report.append("æ„ç¾©: è¡¡é‡å¸‚å ´çš„è³‡è¨Šæ•ˆç‡")
    report.append("è§£è®€:")
    report.append("â€¢ æ¥è¿‘1: å¸‚å ´æ•ˆç‡è¼ƒé«˜ï¼Œåƒ¹æ ¼å……åˆ†åæ˜ è³‡è¨Š")
    report.append("â€¢ é é›¢1: å¸‚å ´æ•ˆç‡è¼ƒä½ï¼Œå¯èƒ½å­˜åœ¨å¥—åˆ©æ©Ÿæœƒ")
    report.append("")
    
    # æ·»åŠ å¸‚å ´åˆ†æçµè«–
    report.append("ğŸ“ˆ å¸‚å ´åˆ†æçµè«–")
    report.append("-" * 40)
    report.append("")
    
    # åˆ†ææ•´é«”å¸‚å ´ç‰¹æ€§
    avg_volatility = report_df['Volatility_Ann'].mean() if 'Volatility_Ann' in report_df.columns else None
    avg_vr = report_df['VarianceRatio'].mean() if 'VarianceRatio' in report_df.columns else None
    avg_autocorr = report_df['Autocorr_Lag1'].mean() if 'Autocorr_Lag1' in report_df.columns else None
    avg_me = report_df['Market_Efficiency'].mean() if 'Market_Efficiency' in report_df.columns else None
    
    report.append("ğŸ¯ æ•´é«”å¸‚å ´ç‰¹æ€§:")
    if avg_volatility and not pd.isna(avg_volatility):
        volatility_level = "é«˜" if avg_volatility > 0.5 else "ä¸­" if avg_volatility > 0.3 else "ä½"
        report.append(f"1. æ³¢å‹•æ€§: {cfg.symbol}å¹´åŒ–æ³¢å‹•ç‡ç´„{avg_volatility:.1%}ï¼Œå±¬æ–¼{volatility_level}æ³¢å‹•è³‡ç”¢")
    
    if avg_me and not pd.isna(avg_me):
        efficiency_level = "é«˜" if abs(avg_me - 1) < 0.1 else "ä¸­" if abs(avg_me - 1) < 0.2 else "ä½"
        report.append(f"2. å¸‚å ´æ•ˆç‡: å„æ™‚é–“æ¡†æ¶çš„å¸‚å ´æ•ˆç‡æ¯”ç‡éƒ½æ¥è¿‘1ï¼Œé¡¯ç¤ºå¸‚å ´è³‡è¨Šæ•ˆç‡è¼ƒ{efficiency_level}")
    
    if avg_autocorr and not pd.isna(avg_autocorr):
        if avg_autocorr < -0.01:
            report.append("3. å‡å€¼å›æ­¸: å¤§éƒ¨åˆ†æ™‚é–“æ¡†æ¶é¡¯ç¤ºè² è‡ªç›¸é—œï¼Œè¡¨ç¤ºåƒ¹æ ¼å…·æœ‰å‡å€¼å›æ­¸ç‰¹æ€§")
        elif avg_autocorr > 0.01:
            report.append("3. è¶¨å‹¢æ€§: å¤§éƒ¨åˆ†æ™‚é–“æ¡†æ¶é¡¯ç¤ºæ­£è‡ªç›¸é—œï¼Œè¡¨ç¤ºåƒ¹æ ¼å…·æœ‰è¶¨å‹¢æ€§")
        else:
            report.append("3. éš¨æ©Ÿæ€§: å¤§éƒ¨åˆ†æ™‚é–“æ¡†æ¶é¡¯ç¤ºæ¥è¿‘é›¶çš„è‡ªç›¸é—œï¼Œè¡¨ç¤ºåƒ¹æ ¼è®Šå‹•æ¥è¿‘éš¨æ©Ÿ")
    
    # åˆ†æååº¦ç‰¹æ€§
    long_term_timeframes = ['1d', '1w']
    long_term_skewness = []
    for tf in long_term_timeframes:
        if tf in report_df['Timeframe'].values:
            row = report_df[report_df['Timeframe'] == tf].iloc[0]
            if 'Skewness' in row and not pd.isna(row['Skewness']):
                long_term_skewness.append(row['Skewness'])
    
    if long_term_skewness:
        avg_long_skew = sum(long_term_skewness) / len(long_term_skewness)
        if avg_long_skew > 0.1:
            report.append("4. é•·æœŸæ­£ååº¦: è¼ƒé•·æ™‚é–“æ¡†æ¶é¡¯ç¤ºæ­£ååº¦ï¼Œé•·æœŸä¾†çœ‹ä¸Šæ¼²æ©Ÿç‡è¼ƒé«˜")
        elif avg_long_skew < -0.1:
            report.append("4. é•·æœŸè² ååº¦: è¼ƒé•·æ™‚é–“æ¡†æ¶é¡¯ç¤ºè² ååº¦ï¼Œé•·æœŸä¾†çœ‹ä¸‹è·Œæ©Ÿç‡è¼ƒé«˜")
        else:
            report.append("4. é•·æœŸå°ç¨±æ€§: è¼ƒé•·æ™‚é–“æ¡†æ¶é¡¯ç¤ºæ¥è¿‘å°ç¨±çš„åˆ†å¸ƒ")
    
    report.append("")
    report.append("ğŸ¯ äº¤æ˜“ç­–ç•¥å»ºè­°:")
    
    # åˆ†æé€šéC/Aæ¸¬è©¦çš„æ™‚é–“æ¡†æ¶
    passed_timeframes = report_df[report_df['Pass_CA_0.25'] == True]['Timeframe'].tolist()
    short_timeframes = [tf for tf in passed_timeframes if tf in ['1m', '5m', '15m']]
    medium_timeframes = [tf for tf in passed_timeframes if tf in ['1h', '4h']]
    long_timeframes = [tf for tf in passed_timeframes if tf in ['1d', '1w']]
    
    if short_timeframes:
        report.append(f"1. çŸ­ç·šäº¤æ˜“({', '.join(short_timeframes)}): é©åˆï¼Œæˆæœ¬æ•ˆç‡è¼ƒå¥½")
    else:
        report.append("1. çŸ­ç·šäº¤æ˜“(1m-15m): ä¸å»ºè­°ï¼Œå› ç‚ºäº¤æ˜“æˆæœ¬ç›¸å°æ³¢å‹•ç‡éé«˜(C/A > 0.25)")
    
    if medium_timeframes:
        report.append(f"2. ä¸­ç·šäº¤æ˜“({', '.join(medium_timeframes)}): é©åˆï¼Œæˆæœ¬æ•ˆç‡è¼ƒå¥½")
    
    if long_timeframes:
        report.append(f"3. é•·ç·šäº¤æ˜“({', '.join(long_timeframes)}): æœ€é©åˆï¼Œæˆæœ¬æ•ˆç‡æœ€ä½³ï¼Œé©åˆè¶¨å‹¢è·Ÿéš¨ç­–ç•¥")
    
    report.append("")
    report.append("ğŸ¯ é¢¨éšªç®¡ç†å»ºè­°:")
    report.append("1. ç”±æ–¼é«˜æ³¢å‹•æ€§ï¼Œå»ºè­°ä½¿ç”¨è¼ƒå°çš„å€‰ä½è¦æ¨¡")
    report.append("2. è¨­ç½®é©ç•¶çš„æ­¢æä½ï¼Œé¿å…æ¥µç«¯åƒ¹æ ¼è®Šå‹•é€ æˆçš„æå¤±")
    report.append("3. è€ƒæ…®ä½¿ç”¨æœŸæ¬Šç­‰è¡ç”Ÿå“é€²è¡Œé¢¨éšªå°æ²–")
    report.append("4. é—œæ³¨å¸‚å ´æƒ…ç·’æŒ‡æ¨™ï¼Œé¿å…åœ¨æ¥µç«¯å¸‚å ´æ¢ä»¶ä¸‹äº¤æ˜“")
    
    report.append("")
    report.append("ğŸ¯ æœ€ä½³æ™‚é–“æ¡†æ¶é¸æ“‡:")
    
    # æ‰¾å‡ºæœ€ä½³æ™‚é–“æ¡†æ¶
    if best_ca is not None and not pd.isna(best_ca['C_over_A']):
        if best_ca['Timeframe'] in ['1h', '4h']:
            report.append(f"â€¢ æ—¥å…§äº¤æ˜“: {best_ca['Timeframe']}æ™‚é–“æ¡†æ¶ (C/A: {best_ca['C_over_A']:.4f})")
        elif best_ca['Timeframe'] in ['1d', '1w']:
            report.append(f"â€¢ é•·æœŸæŠ•è³‡: {best_ca['Timeframe']}æ™‚é–“æ¡†æ¶ (C/A: {best_ca['C_over_A']:.4f}ï¼Œæˆæœ¬æ•ˆç‡æœ€ä½³)")
    
    if best_vr is not None and not pd.isna(best_vr['VarianceRatio']):
        if best_vr['VarianceRatio'] > 1.05:
            report.append(f"â€¢ æ³¢æ®µäº¤æ˜“: {best_vr['Timeframe']}æ™‚é–“æ¡†æ¶ (VR: {best_vr['VarianceRatio']:.4f}ï¼Œè¶¨å‹¢æ€§æœ€å¼·)")
    
    report.append("")
    report.append("=" * 80)
    
    return "\n".join(report)


def generate_md_report(report_df: pd.DataFrame, cfg: Config, df_1m: pd.DataFrame) -> str:
    """
    ç”ŸæˆMarkdownæ ¼å¼çš„è©³ç´°å ±å‘Š
    
    Args:
        report_df: æ™‚é–“æ¡†æ¶åˆ†æçµæœDataFrame
        cfg: é…ç½®ç‰©ä»¶
        df_1m: åŸå§‹1åˆ†é˜è³‡æ–™
    
    Returns:
        Markdownæ ¼å¼å ±å‘Šæ–‡å­—
    """
    report = []
    report.append("# ETHUSDT æ™‚é–“æ¡†æ¶é¸æ“‡åˆ†æå ±å‘Š")
    report.append("")
    report.append(f"**ç”Ÿæˆæ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append("")
    
    # åŸºæœ¬è³‡è¨Š
    report.append("## ğŸ“Š åŸºæœ¬è³‡è¨Š")
    report.append("")
    report.append("| é …ç›® | æ•¸å€¼ |")
    report.append("|------|------|")
    report.append(f"| äº¤æ˜“å° | {cfg.symbol} |")
    report.append(f"| äº¤æ˜“æ‰€ | {cfg.exchange.upper()} |")
    report.append(f"| æ¸¬è©¦é–‹å§‹æ™‚é–“ | {df_1m.index.min().strftime('%Y-%m-%d %H:%M:%S')} |")
    report.append(f"| æ¸¬è©¦çµæŸæ™‚é–“ | {df_1m.index.max().strftime('%Y-%m-%d %H:%M:%S')} |")
    report.append(f"| ç¸½æ¸¬è©¦å¤©æ•¸ | {(df_1m.index.max() - df_1m.index.min()).days} å¤© |")
    report.append(f"| åŸå§‹è³‡æ–™Kç·šæ•¸ | {len(df_1m):,} |")
    report.append("")
    
    # æˆæœ¬è¨­å®š
    cost_one_way = (cfg.taker_fee if cfg.use_taker else cfg.maker_fee) + cfg.slippage_bps / 10000.0
    report.append("## ğŸ’° æˆæœ¬è¨­å®š")
    report.append("")
    report.append("| é …ç›® | æ•¸å€¼ |")
    report.append("|------|------|")
    report.append(f"| è²»ç‡é¡å‹ | {'åƒå–®è²»ç‡' if cfg.use_taker else 'æ›å–®è²»ç‡'} |")
    report.append(f"| å–®é‚Šæˆæœ¬ | {cost_one_way:.6f} ({cost_one_way*100:.4f}%) |")
    report.append(f"| ä¾†å›æˆæœ¬ | {cost_one_way*2:.6f} ({cost_one_way*2*100:.4f}%) |")
    report.append("")
    
    # åˆ†æè¨­å®š
    report.append("## âš™ï¸ åˆ†æè¨­å®š")
    report.append("")
    report.append("| é …ç›® | æ•¸å€¼ |")
    report.append("|------|------|")
    report.append(f"| ATRé€±æœŸ | {cfg.atr_period} |")
    report.append(f"| Variance Ratioèšåˆå°ºåº¦ | {cfg.vr_q} |")
    report.append(f"| åŠè¡°æœŸæœ€å¤§å»¶é² | {cfg.half_life_max_lag} |")
    report.append("")
    
    # æ™‚é–“æ¡†æ¶åˆ†æçµæœ
    report.append("## ğŸ“ˆ æ™‚é–“æ¡†æ¶åˆ†æçµæœ")
    report.append("")
    
    # å‰µå»ºå·²æ¸¬è©¦æ™‚é–“æ¡†æ¶çš„é›†åˆ
    tested_timeframes = set(report_df['Timeframe'].tolist())
    
    # å»ºç«‹çµæœè¡¨æ ¼
    table_headers = ["æ™‚é–“æ¡†æ¶", "Kç·šæ•¸", "C/A", "VR", "åŠè¡°æœŸ", "æ³¢å‹•ç‡", "ååº¦", "å³°åº¦", "è‡ªç›¸é—œ", "å¸‚å ´æ•ˆç‡", "é€šéC/Aæ¸¬è©¦", "ç‹€æ…‹"]
    report.append("| " + " | ".join(table_headers) + " |")
    report.append("|" + "|".join(["---"] * len(table_headers)) + "|")
    
    # éæ­·æ‰€æœ‰é…ç½®çš„æ™‚é–“æ¡†æ¶
    for tf_label, rule in cfg.timeframes.items():
        if tf_label in tested_timeframes:
            # æ‰¾åˆ°å°æ‡‰çš„æ¸¬è©¦çµæœ
            row = report_df[report_df['Timeframe'] == tf_label].iloc[0]
            volatility = f"{row['Volatility_Ann']:.4f}" if 'Volatility_Ann' in row and not pd.isna(row['Volatility_Ann']) else "N/A"
            skewness = f"{row['Skewness']:.4f}" if 'Skewness' in row and not pd.isna(row['Skewness']) else "N/A"
            kurtosis = f"{row['Kurtosis']:.4f}" if 'Kurtosis' in row and not pd.isna(row['Kurtosis']) else "N/A"
            autocorr = f"{row['Autocorr_Lag1']:.4f}" if 'Autocorr_Lag1' in row and not pd.isna(row['Autocorr_Lag1']) else "N/A"
            market_eff = f"{row['Market_Efficiency']:.4f}" if 'Market_Efficiency' in row and not pd.isna(row['Market_Efficiency']) else "N/A"
            pass_ca = "âœ…" if row['Pass_CA_0.25'] else "âŒ"
            
            table_row = [
                row['Timeframe'],
                f"{row['Bars']:,}",
                f"{row['C_over_A']:.4f}",
                f"{row['VarianceRatio']:.4f}",
                f"{row['HalfLife_bars']:.1f}",
                volatility,
                skewness,
                kurtosis,
                autocorr,
                market_eff,
                pass_ca,
                "âœ… å·²æ¸¬è©¦"
            ]
        else:
            # æœªæ¸¬è©¦çš„æ™‚é–“æ¡†æ¶
            table_row = [
                tf_label,
                "N/A",
                "N/A",
                "N/A",
                "N/A",
                "N/A",
                "N/A",
                "N/A",
                "N/A",
                "N/A",
                "N/A",
                "âŒ æœªæ¸¬è©¦"
            ]
        report.append("| " + " | ".join(table_row) + " |")
    
    report.append("")
    
    # è©³ç´°åˆ†æ
    report.append("### ğŸ” è©³ç´°åˆ†æ")
    report.append("")
    
    # éæ­·æ‰€æœ‰é…ç½®çš„æ™‚é–“æ¡†æ¶
    for tf_label, rule in cfg.timeframes.items():
        if tf_label in tested_timeframes:
            # æ‰¾åˆ°å°æ‡‰çš„æ¸¬è©¦çµæœ
            row = report_df[report_df['Timeframe'] == tf_label].iloc[0]
            report.append(f"#### ğŸ• {row['Timeframe']} æ™‚é–“æ¡†æ¶")
            report.append("")
            report.append(f"**åŸºæœ¬çµ±è¨ˆ:**")
            report.append(f"- Kç·šæ•¸é‡: {row['Bars']:,}")
            report.append(f"- å¹³å‡ATR: {row['Avg_ATR_pct']:.4f} ({row['Avg_ATR_pct']*100:.2f}%)")
            report.append(f"- æˆæœ¬/æ³¢å‹•æ¯” (C/A): {row['C_over_A']:.4f}")
            report.append(f"- èµ°å‹¢ä¸€è‡´æ€§ (VR): {row['VarianceRatio']:.4f}")
            report.append(f"- è¨Šè™ŸåŠè¡°æœŸ: {row['HalfLife_bars']:.1f} bars")
            report.append("")
            
            # çµ±è¨ˆç‰¹æ€§
            if 'Volatility_Ann' in row and not pd.isna(row['Volatility_Ann']):
                report.append(f"**çµ±è¨ˆç‰¹æ€§:**")
                report.append(f"- å¹´åŒ–æ³¢å‹•ç‡: {row['Volatility_Ann']:.4f}")
                report.append(f"- å ±é…¬ååº¦: {row['Skewness']:.4f}")
                report.append(f"- å ±é…¬å³°åº¦: {row['Kurtosis']:.4f}")
                report.append(f"- è‡ªç›¸é—œ(Lag1): {row['Autocorr_Lag1']:.4f}")
                report.append(f"- å¸‚å ´æ•ˆç‡æ¯”ç‡: {row['Market_Efficiency']:.4f}")
                report.append("")
            
            # å¯è¡Œæ€§è©•ä¼°
            if row['Pass_CA_0.25']:
                report.append("**å¯è¡Œæ€§è©•ä¼°:** âœ… é€šéC/A < 0.25æ¸¬è©¦")
            else:
                report.append("**å¯è¡Œæ€§è©•ä¼°:** âŒ æœªé€šéC/A < 0.25æ¸¬è©¦")
        else:
            # æœªæ¸¬è©¦çš„æ™‚é–“æ¡†æ¶
            report.append(f"#### ğŸ• {tf_label} æ™‚é–“æ¡†æ¶")
            report.append("")
            report.append("**ç‹€æ…‹:** âŒ æœªé€²è¡Œæ¸¬è©¦")
            report.append("")
            report.append("**åŸå› :**")
            min_bars_required = get_min_bars_for_timeframe(tf_label, cfg)
            min_days_required = cfg.min_days_per_timeframe.get(tf_label, 365) if cfg.use_dynamic_min_bars else "N/A"
            report.append(f"- è³‡æ–™é‡ä¸è¶³ï¼ˆéœ€è¦è‡³å°‘ {min_bars_required} æ ¹Kç·šï¼‰")
            if cfg.use_dynamic_min_bars:
                report.append(f"- è¦æ±‚è‡³å°‘ {min_days_required} å¤©çš„è³‡æ–™")
            report.append("")
            report.append("**å»ºè­°:**")
            report.append("- å¢åŠ è³‡æ–™å¤©æ•¸")
            report.append("- æˆ–èª¿æ•´æœ€å°è³‡æ–™é‡è¨­å®š")
        
        report.append("")
    
    # ç¶œåˆå»ºè­°
    report.append("## ğŸ’¡ ç¶œåˆå»ºè­°")
    report.append("")
    
    # æ‰¾å‡ºæœ€ä½³æ™‚é–“æ¡†æ¶
            best_ca = report_df.loc[report_df['C_over_A'].idxmin()] if 'C_over_A' in report_df.columns else None
        best_vr = report_df.loc[report_df['VarianceRatio'].idxmax()] if 'VarianceRatio' in report_df.columns else None
        
        if best_ca is not None and not pd.isna(best_ca['C_over_A']):
            report.append(f"**æœ€ä½³æˆæœ¬æ•ˆç‡æ™‚é–“æ¡†æ¶:** {best_ca['Timeframe']} (C/A: {best_ca['C_over_A']:.4f})")
        
        if best_vr is not None and not pd.isna(best_vr['VarianceRatio']):
            report.append(f"**æœ€é«˜è¶¨å‹¢æ€§æ™‚é–“æ¡†æ¶:** {best_vr['Timeframe']} (VR: {best_vr['VarianceRatio']:.4f})")
    
    report.append("")
    report.append("### ğŸ“‹ æŒ‡æ¨™è§£è®€æŒ‡å—")
    report.append("")
    report.append("| æŒ‡æ¨™ | è§£è®€ |")
    report.append("|------|------|")
    report.append("| C/A < 0.25 | æˆæœ¬ç›¸å°æ–¼æ³¢å‹•ç‡è¼ƒä½ï¼Œé©åˆäº¤æ˜“ |")
    report.append("| VR > 1 | åè¶¨å‹¢å¸‚å ´ï¼Œé©åˆè¶¨å‹¢ç­–ç•¥ |")
    report.append("| VR < 1 | åå‡å€¼å›æ­¸å¸‚å ´ï¼Œé©åˆå‡å€¼å›æ­¸ç­–ç•¥ |")
    report.append("| åŠè¡°æœŸ | å»ºè­°baré€±æœŸç´„ç‚º0.5~1å€åŠè¡°æœŸ |")
    report.append("| æ¨£æœ¬å¤–å¤æ™®æ¯”ç‡ | è¶Šé«˜è¶Šå¥½ï¼Œè¡¨ç¤ºç­–ç•¥ç©©å®šæ€§ |")
    report.append("| æ¨£æœ¬å¤–æœ€å¤§å›æ’¤ | è¶Šä½è¶Šå¥½ï¼Œè¡¨ç¤ºé¢¨éšªæ§åˆ¶ |")
    report.append("")
    
    return "\n".join(report)


# =======================
# ====== UTILITIES ======
# =======================

def detect_columns(df: pd.DataFrame, cfg: Config) -> Dict[str, str]:
    """å˜—è©¦ä»¥å¯¬é¬†è¦å‰‡å°æ˜ æ¬„ä½åã€‚"""
    cols = {c.lower(): c for c in df.columns}
    mapping = {}

    def pick(cands):
        for c in cands:
            if c in cols:
                return cols[c]
        return None

    mapping['ts'] = pick([cfg.ts_col.lower(), 'timestamp', 'date', 'time', 'datetime'])
    mapping['open'] = pick([cfg.open_col.lower(), 'open', 'o'])
    mapping['high'] = pick([cfg.high_col.lower(), 'high', 'h'])
    mapping['low'] = pick([cfg.low_col.lower(), 'low', 'l'])
    mapping['close'] = pick([cfg.close_col.lower(), 'close', 'c', 'price'])
    mapping['vol'] = pick([cfg.vol_col.lower(), 'volume', 'vol', 'v'])

    missing = [k for k, v in mapping.items() if v is None and k != 'vol']  # volume å¯é¸
    if missing:
        raise ValueError(f"æ‰¾ä¸åˆ°å¿…è¦æ¬„ä½ï¼ˆå¤§å°å¯«ä¸æ‹˜ï¼‰: {missing}ï¼Œè«‹æª¢æŸ¥ CSV æ¬„åã€‚")

    return mapping


def load_1m_csv(path: str, cfg: Config) -> pd.DataFrame:
    """è®€å– 1m CSVï¼Œæ¨™æº–åŒ–æ¬„ä½ï¼Œè¨­å®šç‚ºæ™‚åºç´¢å¼•ï¼ˆUTCï¼‰ã€‚"""
    df = pd.read_csv(path)
    mapping = detect_columns(df, cfg)

    ts = pd.to_datetime(df[mapping['ts']], unit='ms', errors='coerce')
    if ts.isna().mean() > 0.5:
        # å¤šåŠä»£è¡¨ä¸æ˜¯æ¯«ç§’ï¼Œæ”¹ç”¨è‡ªç„¶è§£æ
        ts = pd.to_datetime(df[mapping['ts']], errors='coerce')

    if ts.isna().any():
        df = df.loc[~ts.isna()].copy()
        ts = pd.to_datetime(df[mapping['ts']], errors='coerce')
        if ts.isna().any():
            raise ValueError("timestamp æ¬„ä½è§£æå¤±æ•—ï¼Œè«‹ç¢ºèªæ ¼å¼ã€‚")

    # è™•ç†æ™‚å€
    if ts.dt.tz is None:
        ts = ts.dt.tz_localize('UTC', nonexistent='shift_forward', ambiguous='NaT')
    else:
        ts = ts.dt.tz_convert('UTC')
    
    # è½‰æ›åˆ°ç›®æ¨™æ™‚å€
    import pytz
    target_tz = pytz.timezone(cfg.tz) if isinstance(cfg.tz, str) else cfg.tz
    ts = ts.dt.tz_convert(target_tz)
    
    df = df.assign(
        timestamp=ts,
        open=df[mapping['open']].astype(float),
        high=df[mapping['high']].astype(float),
        low=df[mapping['low']].astype(float),
        close=df[mapping['close']].astype(float),
        volume=df[mapping['vol']].astype(float) if mapping['vol'] is not None else np.nan,
    ).dropna(subset=['open', 'high', 'low', 'close'])

    df = df.drop_duplicates(subset=['timestamp']).set_index('timestamp').sort_index()
    # å˜—è©¦è£œé½Šç¼ºæ¼åˆ†é˜ï¼ˆå¯é¸ï¼‰
    # all_minutes = pd.date_range(df.index.min(), df.index.max(), freq='1T', tz=cfg.tz)
    # df = df.reindex(all_minutes).ffill()
    return df


def resample_ohlcv(df_1m: pd.DataFrame, rule: str) -> pd.DataFrame:
    """ä»¥ OHLCV è¦å‰‡é‡æ¡æ¨£ã€‚"""
    agg = {
        'open': 'first',
        'high': 'max',
        'low': 'min',
        'close': 'last',
        'volume': 'sum'
    }
    return df_1m.resample(rule, label='right', closed='right').agg(agg).dropna(subset=['open', 'high', 'low', 'close'])


def compute_atr(df: pd.DataFrame, period: int) -> pd.Series:
    """ç°¡æ˜“ ATRï¼ˆSMA ç‰ˆï¼‰ã€‚"""
    high = df['high']
    low = df['low']
    close = df['close']
    prev_close = close.shift(1)

    tr = pd.concat([
        (high - low),
        (high - prev_close).abs(),
        (low - prev_close).abs()
    ], axis=1).max(axis=1)

    atr = tr.rolling(period).mean()
    return atr


def variance_ratio(returns: pd.Series, q: int) -> float:
    """è¨ˆç®— Lo-MacKinlay å‹çš„ç°¡åŒ– Variance Ratioã€‚"""
    r = returns.dropna()
    if len(r) < q + 2:
        return np.nan
    var_1 = np.var(r, ddof=1)
    r_q = r.rolling(q).sum()
    var_q = np.var(r_q.dropna(), ddof=1)
    if var_1 == 0:
        return np.nan
    return float(var_q / (q * var_1))


def estimate_half_life_by_autocorr(returns: pd.Series, max_lag: int) -> Optional[float]:
    """
    ç”¨å ±é…¬è‡ªç›¸é—œçš„è¡°æ¸›ä¾†è¿‘ä¼¼ã€Œè¨Šè™ŸåŠè¡°æœŸã€ï¼š
    æ‰¾å‡º lag=1 çš„è‡ªç›¸é—œ rho1ï¼Œå¾€å¾Œå°‹æ‰¾ç¬¬ä¸€å€‹ lag=k ä½¿å¾— |rho_k| <= 0.5*|rho1|ã€‚
    å›å‚³ kï¼ˆå–®ä½ï¼šbarï¼‰ã€‚è‹¥ rho1 ç„¡æ„ç¾©æˆ–æ‰¾ä¸åˆ°ï¼Œå›å‚³ Noneã€‚
    """
    r = returns.dropna()
    if len(r) < max_lag + 5:
        return None

    r = r - r.mean()
    var = (r ** 2).sum()
    if var == 0:
        return None

    def autocorr(lag: int) -> float:
        return float((r[lag:] * r.shift(lag)[lag:]).sum() / var)

    rho1 = autocorr(1)
    if np.isnan(rho1) or abs(rho1) < 1e-6:
        return None

    target = 0.5 * abs(rho1)
    for k in range(2, max_lag + 1):
        rhok = autocorr(k)
        if np.isnan(rhok):
            continue
        if abs(rhok) <= target:
            return float(k)
    return None


def bar_minutes(label: str) -> float:
    mapping = {
        "1m": 1, "5m": 5, "15m": 15,
        "1h": 60, "4h": 240,
        "1d": 1440, "1w": 10080
    }
    return float(mapping.get(label, 1.0))


def get_min_bars_for_timeframe(tf_label: str, cfg: Config) -> int:
    """
    æ ¹æ“šæ™‚é–“æ¡†æ¶å‹•æ…‹è¨ˆç®—æœ€å°è³‡æ–™é‡è¦æ±‚
    
    Args:
        tf_label: æ™‚é–“æ¡†æ¶æ¨™ç±¤ (å¦‚ "1m", "1h", "4h")
        cfg: é…ç½®ç‰©ä»¶
    
    Returns:
        è©²æ™‚é–“æ¡†æ¶éœ€è¦çš„æœ€å°Kç·šæ•¸é‡
    """
    if not cfg.use_dynamic_min_bars:
        return cfg.n_min_bars_for_backtest
    
    # ç²å–è©²æ™‚é–“æ¡†æ¶çš„æœ€å°å¤©æ•¸è¦æ±‚
    min_days = cfg.min_days_per_timeframe.get(tf_label, 365)
    
    # è¨ˆç®—è©²æ™‚é–“æ¡†æ¶æ¯æ ¹Kç·šçš„åˆ†é˜æ•¸
    minutes_per_bar = bar_minutes(tf_label)
    
    # è¨ˆç®—æœ€å°Kç·šæ•¸é‡
    min_bars = int((min_days * 24 * 60) / minutes_per_bar)
    
    # ç¢ºä¿è‡³å°‘æœ‰100æ ¹Kç·šä½œç‚ºçµ•å°æœ€å°å€¼
    min_bars = max(min_bars, 100)
    
    return min_bars


def annualization_factor(tf_label: str) -> float:
    """ä»¥ 365*24*60 åˆ†é˜/å¹´ æ›ç®—æ¯å€‹ bar çš„å¹´åŒ–å€æ•¸ã€‚"""
    bars_per_year = (365.0 * 24.0 * 60.0) / bar_minutes(tf_label)
    return bars_per_year


# =======================
# ====== æŠ€è¡“æŒ‡æ¨™è¨ˆç®— =======
# =======================

def calculate_volatility(returns: pd.Series, ann_factor: float) -> float:
    """è¨ˆç®—å¹´åŒ–æ³¢å‹•ç‡"""
    r = returns.dropna()
    if len(r) < 2:
        return np.nan
    return float(r.std(ddof=1) * math.sqrt(ann_factor))


def calculate_skewness(returns: pd.Series) -> float:
    """è¨ˆç®—å ±é…¬ååº¦"""
    r = returns.dropna()
    if len(r) < 3:
        return np.nan
    return float(r.skew())


def calculate_kurtosis(returns: pd.Series) -> float:
    """è¨ˆç®—å ±é…¬å³°åº¦"""
    r = returns.dropna()
    if len(r) < 4:
        return np.nan
    return float(r.kurtosis())


def calculate_autocorrelation(returns: pd.Series, lag: int = 1) -> float:
    """è¨ˆç®—å ±é…¬è‡ªç›¸é—œ"""
    r = returns.dropna()
    if len(r) < lag + 1:
        return np.nan
    return float(r.autocorr(lag=lag))


def calculate_market_efficiency_ratio(returns: pd.Series) -> float:
    """è¨ˆç®—å¸‚å ´æ•ˆç‡æ¯”ç‡ï¼ˆåŸºæ–¼æ–¹å·®æ¯”ï¼‰"""
    r = returns.dropna()
    if len(r) < 10:
        return np.nan
    
    # è¨ˆç®—ä¸åŒæ™‚é–“é–“éš”çš„æ–¹å·®æ¯”
    var_1 = r.var()
    var_2 = r.rolling(2).sum().dropna().var()
    
    if var_1 == 0:
        return np.nan
    
    return float(var_2 / (2 * var_1))


# =======================
# ====== PIPELINE =======
# =======================

def main(cfg: Config):
    print("=== æ™‚é–“æ¡†æ¶é¸æ“‡å·¥å…· ===")
    
    try:
        df_1m = smart_data_loader(cfg)
        print(generate_data_report(df_1m, cfg))
    except Exception as e:
        print("è³‡æ–™è¼‰å…¥å¤±æ•—ï¼š", e)
        print("è«‹ç¢ºèªç¶²è·¯é€£ç·šæˆ–æª¢æŸ¥ CSV è·¯å¾‘èˆ‡æ¬„ä½ã€‚")
        return

    report_rows = []

    # æˆæœ¬ï¼ˆå–®é‚Šï¼‰
    cost_one_way = (cfg.taker_fee if cfg.use_taker else cfg.maker_fee) + cfg.slippage_bps / 10000.0
    print(f"æ¡ç”¨ {'åƒå–®' if cfg.use_taker else 'æ›å–®'} è²»ç‡ï¼›å–®é‚Šæˆæœ¬ = {cost_one_way:.6f} ({cost_one_way*100:.4f}%)")

    for tf_label, rule in cfg.timeframes.items():
        print(f"\n--- æ™‚é–“æ¡†æ¶ï¼š{tf_label} ({rule}) ---")
        ohlc = resample_ohlcv(df_1m, rule)
        
        # å‹•æ…‹è¨ˆç®—è©²æ™‚é–“æ¡†æ¶çš„æœ€å°è³‡æ–™é‡è¦æ±‚
        min_bars_required = get_min_bars_for_timeframe(tf_label, cfg)
        
        if len(ohlc) < min_bars_required:
            min_days_required = cfg.min_days_per_timeframe.get(tf_label, 365) if cfg.use_dynamic_min_bars else "N/A"
            print(f"è³‡æ–™é‡ä¸è¶³ï¼ˆ{len(ohlc)} < {min_bars_required} barsï¼‰ï¼Œç•¥éã€‚")
            if cfg.use_dynamic_min_bars:
                print(f"  è©²æ™‚é–“æ¡†æ¶éœ€è¦è‡³å°‘ {min_days_required} å¤©çš„è³‡æ–™")
            continue

        ann_factor = annualization_factor(tf_label)

        # C/A
        atr = compute_atr(ohlc, cfg.atr_period)
        atr_pct = (atr / ohlc['close']).dropna()
        avg_atr_pct = float(atr_pct.mean()) if len(atr_pct) else np.nan
        cost_roundtrip = 2.0 * cost_one_way
        c_over_a = float(cost_roundtrip / avg_atr_pct) if avg_atr_pct and avg_atr_pct > 0 else np.nan

        # VR
        ret = ohlc['close'].pct_change()
        vr = variance_ratio(np.log1p(ret), cfg.vr_q)

        # åŠè¡°æœŸï¼ˆå ±é…¬è‡ªç›¸é—œè¿‘ä¼¼ï¼‰
        hl = estimate_half_life_by_autocorr(np.log1p(ret), cfg.half_life_max_lag)

        # è¨ˆç®—é¡å¤–çš„æŠ€è¡“æŒ‡æ¨™
        volatility = calculate_volatility(ret, ann_factor)
        skewness = calculate_skewness(ret)
        kurtosis = calculate_kurtosis(ret)
        autocorr_1 = calculate_autocorrelation(ret, lag=1)
        market_efficiency = calculate_market_efficiency_ratio(ret)

        row = {
            "Timeframe": tf_label,
            "Bars": len(ohlc),
            "Avg_ATR_pct": avg_atr_pct,
            "Cost_RoundTrip_pct": cost_roundtrip,
            "C_over_A": c_over_a,
            "VR_q": cfg.vr_q,
            "VarianceRatio": vr,
            "HalfLife_bars": hl,
            "Volatility_Ann": volatility,
            "Skewness": skewness,
            "Kurtosis": kurtosis,
            "Autocorr_Lag1": autocorr_1,
            "Market_Efficiency": market_efficiency,
        }

        # ç°¡å–®å¯è¡Œæ€§æ¨™è¨˜
        row["Pass_CA_0.25"] = (c_over_a < 0.25) if not (pd.isna(c_over_a)) else False

        report_rows.append(row)

    if not report_rows:
        print("æ²’æœ‰å¯ç”¨çš„æ™‚é–“æ¡†æ¶çµæœã€‚è«‹ç¢ºèªè³‡æ–™é‡æˆ–èª¿æ•´æœ€å°è³‡æ–™é‡è¨­å®šã€‚")
        if cfg.use_dynamic_min_bars:
            print("\nå„æ™‚é–“æ¡†æ¶çš„æœ€å°è³‡æ–™é‡è¦æ±‚ï¼š")
            for tf_label in cfg.timeframes.keys():
                min_bars = get_min_bars_for_timeframe(tf_label, cfg)
                min_days = cfg.min_days_per_timeframe.get(tf_label, 365)
                print(f"  {tf_label}: {min_bars} æ ¹Kç·š ({min_days} å¤©)")
        return

    report = pd.DataFrame(report_rows).sort_values(
        by=["Pass_CA_0.25", "C_over_A", "VarianceRatio"],
        ascending=[False, True, False]
    )

    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    os.makedirs("./data", exist_ok=True)
    
    # ç”ŸæˆåŒ…å«æ—¥æœŸå€é–“çš„æª”å
    start_date = df_1m.index.min().strftime('%Y%m%d')
    end_date = df_1m.index.max().strftime('%Y%m%d')
    date_range = f"{start_date}-{end_date}"
    
    # ç”ŸæˆCSVå ±å‘Š
    if cfg.generate_csv_report:
        out_csv = f"./data/ethusdt_timeframe_report_{date_range}.csv"
        report.to_csv(out_csv, index=False, encoding="utf-8-sig")
        print(f"\nâœ… å·²è¼¸å‡ºCSVå ±è¡¨ï¼š{out_csv}")
    
    # ç”ŸæˆTXTå ±å‘Š
    if cfg.generate_txt_report:
        txt_report = generate_txt_report(report, cfg, df_1m)
        out_txt = f"./data/ethusdt_timeframe_report_{date_range}.txt"
        with open(out_txt, 'w', encoding='utf-8') as f:
            f.write(txt_report)
        print(f"âœ… å·²è¼¸å‡ºTXTå ±è¡¨ï¼š{out_txt}")
    
    # ç”ŸæˆMDå ±å‘Š
    if cfg.generate_md_report:
        md_report = generate_md_report(report, cfg, df_1m)
        out_md = f"./data/ethusdt_timeframe_report_{date_range}.md"
        with open(out_md, 'w', encoding='utf-8') as f:
            f.write(md_report)
        print(f"âœ… å·²è¼¸å‡ºMDå ±è¡¨ï¼š{out_md}")
    
    print("\nğŸ“‹ å ±å‘Šè§£è®€æŒ‡å—ï¼š")
    print("1) å…ˆçœ‹ C_over_A æ˜¯å¦ < 0.25ï¼ˆè¶Šä½è¶Šå¥½ï¼Œè¡¨ç¤ºäº¤æ˜“æˆæœ¬ç›¸å°æ³¢å‹•è¼ƒä½ï¼‰ã€‚")
    print("2) VarianceRatio > 1 è¡¨ç¤ºåè¶¨å‹¢å¸‚å ´ï¼Œé©åˆè¶¨å‹¢ç­–ç•¥ï¼›< 1 è¡¨ç¤ºåå‡å€¼å›æ­¸å¸‚å ´ã€‚")
    print("3) HalfLife_bars æç¤ºé©åˆçš„baré€±æœŸï¼ˆå»ºè­°baré€±æœŸç´„ç‚º0.5~1å€åŠè¡°æœŸï¼‰ã€‚")
    print("4) æ³¢å‹•ç‡ã€ååº¦ã€å³°åº¦ç­‰æŒ‡æ¨™åæ˜ å¸‚å ´ç‰¹æ€§ã€‚")
    print("5) è©³ç´°å ±å‘ŠåŒ…å«æ¸¬è©¦æ—¥æœŸç¯„åœã€æˆæœ¬è¨­å®šç­‰å®Œæ•´è³‡è¨Šã€‚")


if __name__ == "__main__":
    main(CFG)
